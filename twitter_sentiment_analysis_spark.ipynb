{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Malek-Ghorbel/TwitterSentimentAnalysis-BigData/blob/main/twitter_sentiment_analysis_spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Installing packages**\n"
      ],
      "metadata": {
        "id": "pZ5vjbfUAnZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbtx09sjDGAW",
        "outputId": "35127107-0fe0-4737-e35e-560800db98b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.2.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.2-py2.py3-none-any.whl size=281824028 sha256=8367c2d714fc4391b3fb145f7325eb8cc913b806e73b807614aae3af16157f5f\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/e3/9b/0525ce8a69478916513509d43693511463c6468db0de237c86\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.7\n",
            "    Uninstalling py4j-0.10.9.7:\n",
            "      Successfully uninstalled py4j-0.10.9.7\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sparknlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ld2Era7gDJuZ",
        "outputId": "0776be20-b5bd-44a2-fa38-6697504d7653"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sparknlp\n",
            "  Downloading sparknlp-1.0.0-py3-none-any.whl (1.4 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from sparknlp) (1.22.4)\n",
            "Collecting spark-nlp\n",
            "  Downloading spark_nlp-4.3.2-py2.py3-none-any.whl (473 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m473.2/473.2 KB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: spark-nlp, sparknlp\n",
            "Successfully installed spark-nlp-4.3.2 sparknlp-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**importing necessary libraries**"
      ],
      "metadata": {
        "id": "B-fFb7dRAzVf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7Z6RLHWChRx"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark.sql.functions import * \n",
        "from pyspark.sql.types import * \n",
        "from pyspark.sql import SparkSession \n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import re \n",
        "from pyspark.ml.feature import HashingTF, IDF, StringIndexer, SQLTransformer,IndexToString,CountVectorizer \n",
        "\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml import Pipeline ,PipelineModel\n",
        "\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator \n",
        "\n",
        "import sparknlp\n",
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp import DocumentAssembler\n",
        "\n",
        "\n",
        "import os\n",
        "import gc"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialization of spark session**"
      ],
      "metadata": {
        "id": "PcX9WXlSA9sJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession #Import the spark session\n",
        "from pyspark import SparkContext #Create a spark context\n",
        "from pyspark.sql import SQLContext #Create an SQL context\n",
        "\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Spark NLP\")\\\n",
        "    .master(\"local[*]\")\\\n",
        "    .config(\"spark.executor.memory\", \"12g\").config(\"spark.driver.memory\", \"12g\")\\\n",
        "    .config(\"spark.memory.offHeap.enabled\",True).config(\"spark.memory.offHeap.size\",\"16g\")\\\n",
        "    .config('spark.executor.cores', '3').config('spark.cores.max', '3')\\\n",
        "    .config(\"spark.driver.maxResultSize\", \"0\") \\\n",
        "    .config(\"spark.kryoserializer.buffer.max\", \"2000M\")\\\n",
        "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:3.2.3\").getOrCreate()"
      ],
      "metadata": {
        "id": "xaqyyiWkDjuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Charging the dataset (exisitng in drive)**\n"
      ],
      "metadata": {
        "id": "ybSNnDv5BHO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mIyDbuhspnc",
        "outputId": "0d8ca3b3-7cb6-4e62-d445-88d01d54c785"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = spark.read.csv(os.getcwd()+\"/MyDrive/training.csv\", inferSchema = True, header = False) #Read in the data\n",
        "training_data.show(10)"
      ],
      "metadata": {
        "id": "c8ffjBFsFM3x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f41cd03d-6784-47b2-dc67-425d46e9a142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+--------------------+--------+---------------+--------------------+\n",
            "|_c0|       _c1|                 _c2|     _c3|            _c4|                 _c5|\n",
            "+---+----------+--------------------+--------+---------------+--------------------+\n",
            "|  0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
            "|  0|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|\n",
            "|  0|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
            "|  0|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|\n",
            "|  0|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...|\n",
            "|  0|1467811372|Mon Apr 06 22:20:...|NO_QUERY|       joy_wolf|@Kwesidei not the...|\n",
            "|  0|1467811592|Mon Apr 06 22:20:...|NO_QUERY|        mybirch|         Need a hug |\n",
            "|  0|1467811594|Mon Apr 06 22:20:...|NO_QUERY|           coZZ|@LOLTrish hey  lo...|\n",
            "|  0|1467811795|Mon Apr 06 22:20:...|NO_QUERY|2Hood4Hollywood|@Tatiana_K nope t...|\n",
            "|  0|1467812025|Mon Apr 06 22:20:...|NO_QUERY|        mimismo|@twittera que me ...|\n",
            "+---+----------+--------------------+--------+---------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Operations for Data preprocessing**"
      ],
      "metadata": {
        "id": "yFFTzTqxBTnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns = [\"target\", \"id\", \"date\", \"flag\", \"user\", \"tweet\"]  \n",
        "\n",
        "training_data = training_data.select(col(\"_c0\").alias(columns[0]), col(\"_c1\").alias(columns[1]), col(\"_c2\").alias(columns[2]),\n",
        "                      col(\"_c3\").alias(columns[3]), col(\"_c4\").alias(columns[4]), col(\"_c5\").alias(columns[5]))\n",
        "training_data.show(10) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwCk-lNrHAJ_",
        "outputId": "eddcdf1a-0896-48b6-fbfb-e9efa3103f5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------+--------------------+--------+---------------+--------------------+\n",
            "|target|        id|                date|    flag|           user|               tweet|\n",
            "+------+----------+--------------------+--------+---------------+--------------------+\n",
            "|     0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
            "|     0|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|\n",
            "|     0|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
            "|     0|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|\n",
            "|     0|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...|\n",
            "|     0|1467811372|Mon Apr 06 22:20:...|NO_QUERY|       joy_wolf|@Kwesidei not the...|\n",
            "|     0|1467811592|Mon Apr 06 22:20:...|NO_QUERY|        mybirch|         Need a hug |\n",
            "|     0|1467811594|Mon Apr 06 22:20:...|NO_QUERY|           coZZ|@LOLTrish hey  lo...|\n",
            "|     0|1467811795|Mon Apr 06 22:20:...|NO_QUERY|2Hood4Hollywood|@Tatiana_K nope t...|\n",
            "|     0|1467812025|Mon Apr 06 22:20:...|NO_QUERY|        mimismo|@twittera que me ...|\n",
            "+------+----------+--------------------+--------+---------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "remove unnecessary columns"
      ],
      "metadata": {
        "id": "XrlZs5wXBdN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = training_data.select('target' ,'tweet')\n",
        "training_data.show(10) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkN7GTEaHT9P",
        "outputId": "81e2804f-21c9-462b-95d4-19446a5528a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------------+\n",
            "|target|               tweet|\n",
            "+------+--------------------+\n",
            "|     0|@switchfoot http:...|\n",
            "|     0|is upset that he ...|\n",
            "|     0|@Kenichan I dived...|\n",
            "|     0|my whole body fee...|\n",
            "|     0|@nationwideclass ...|\n",
            "|     0|@Kwesidei not the...|\n",
            "|     0|         Need a hug |\n",
            "|     0|@LOLTrish hey  lo...|\n",
            "|     0|@Tatiana_K nope t...|\n",
            "|     0|@twittera que me ...|\n",
            "+------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "normalizing the sentiment column values"
      ],
      "metadata": {
        "id": "Iuzw5jKlB4kg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = training_data.withColumn(\"target\", when(training_data[\"target\"] == 4, 1).otherwise(training_data[\"target\"]))\n",
        "training_data.groupBy(\"target\").count().orderBy(\"count\").show()"
      ],
      "metadata": {
        "id": "iv1mXTE3JClq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9dc95eb-793a-4d14-aed4-fdcf85052e7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+\n",
            "|target| count|\n",
            "+------+------+\n",
            "|     1|800000|\n",
            "|     0|800000|\n",
            "+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "remove unneeded words from tweets (mentions, links ...)"
      ],
      "metadata": {
        "id": "YpwKyCGZCEWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = training_data.withColumn('tweet', F.regexp_replace('tweet', r'http\\S+', '')) \n",
        "training_data = training_data.withColumn('tweet', F.regexp_replace('tweet', '@\\w+', '')) \n",
        "training_data = training_data.withColumn('tweet', F.regexp_replace('tweet', '#', ''))\n",
        "training_data = training_data.withColumn('tweet', F.regexp_replace('tweet', 'RT', ''))\n",
        "\n",
        "\n",
        "training_data = training_data.withColumn('tweet', F.regexp_replace('tweet', '&amp;', ''))\n",
        "training_data = training_data.withColumn('tweet', F.regexp_replace('tweet', '&quot;', ''))\n",
        "training_data = training_data.withColumn('tweet', F.regexp_replace('tweet', '&gt;', ''))\n",
        "training_data = training_data.withColumn('tweet', F.regexp_replace('tweet', '&lt;', ''))\n",
        "\n",
        "\n",
        "training_data = training_data.withColumn('tweet', F.regexp_replace('tweet', '-', ''))\n",
        "\n",
        "training_data = training_data.withColumn('tweet', F.regexp_replace('tweet', '   ', ' '))\n",
        "training_data = training_data.withColumn('tweet', F.regexp_replace('tweet', '  ', ' '))\n",
        "\n",
        "\n",
        "training_data = training_data.filter((training_data.tweet!= ' ') &(training_data.tweet!= '')& (training_data.tweet!= '   '))"
      ],
      "metadata": {
        "id": "P7HPD6jFwEG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting data into training and test"
      ],
      "metadata": {
        "id": "fCW5g45SCYdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Train_Test_sets = training_data.randomSplit([0.75, 0.25])\n",
        "train_set = Train_Test_sets[0] \n",
        "test_set = Train_Test_sets[1] "
      ],
      "metadata": {
        "id": "__sol_9zwxAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# document_assembler = DocumentAssembler() \\\n",
        "#     .setInputCol(\"tweet\") \\\n",
        "#     .setOutputCol(\"document\")\n",
        "#document_assembler = DocumentAssembler(inputCol=\"tweet\" , outputCol=\"document\")\n",
        "#model1 = document_assembler.fit(train_set)"
      ],
      "metadata": {
        "id": "XVqDbkb2xC87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dentence_detector = SentenceDetector() \\\n",
        "#     .setInputCols([\"document\"]) \\\n",
        "#     .setOutputCol(\"sentence\")"
      ],
      "metadata": {
        "id": "yW97yectxQrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer = Tokenizer() \\\n",
        "#   .setInputCols([\"sentence\"]) \\\n",
        "#   .setOutputCol(\"token\")"
      ],
      "metadata": {
        "id": "voGWs8QGxYGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stopwords_cleaner = StopWordsCleaner()\\\n",
        "#       .setInputCols(\"token\")\\\n",
        "#       .setOutputCol(\"cleanTokens\")\\\n",
        "#       .setCaseSensitive(False)"
      ],
      "metadata": {
        "id": "uJBEO_saxbe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalizer = Normalizer() \\\n",
        "#     .setInputCols([\"cleanTokens\"]) \\\n",
        "#     .setOutputCol(\"normalized\")\\\n",
        "#     .setLowercase(True)\n",
        "\n",
        "# finisher = Finisher() \\\n",
        "#     .setInputCols([\"normalized\"]) \\\n",
        "#     .setOutputCols([\"token_features\"]) \\\n",
        "#     .setOutputAsArray(True) \\\n",
        "#     .setCleanAnnotations(False)# To generate Term Frequency"
      ],
      "metadata": {
        "id": "s9zypEfzxd3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hashingTF = HashingTF(inputCol=\"token_features\", outputCol=\"rawFeatures\")# To generate Inverse Document Frequency"
      ],
      "metadata": {
        "id": "uOWCeUN3xg-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5)"
      ],
      "metadata": {
        "id": "dDYyuUD7xkKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SVC = LinearSVC(labelCol = \"target\", featuresCol=\"features\",maxIter=13, regParam=0.2)"
      ],
      "metadata": {
        "id": "LXRwkp8qxmD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the stages for the Natural Language Processing (NLP) pipeline that will be applied to data"
      ],
      "metadata": {
        "id": "1u_od-orC4Z9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.common import *\n",
        "from sparknlp.base import *\n",
        "\n",
        "#Turn tweets into documents\n",
        "document_assembler = DocumentAssembler() \\\n",
        "    .setInputCol(\"tweet\") \\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "#Turn these documents into tokens\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "#Normalizing the tokens (Remove punctautions ..)\n",
        "normalizer = Normalizer() \\\n",
        "    .setInputCols([\"token\"]) \\\n",
        "    .setOutputCol(\"normalized\")\n",
        "\n",
        "#Remove stop words from tokens\n",
        "stopwords_cleaner = StopWordsCleaner() \\\n",
        "    .setInputCols([\"normalized\"]) \\\n",
        "    .setOutputCol(\"cleanTokens\") \\\n",
        "    .setCaseSensitive(False)\n",
        "\n",
        "#turn the documented_tokens into array of tokens\n",
        "finisher = Finisher() \\\n",
        "    .setInputCols([\"cleanTokens\"]) \\\n",
        "    .setOutputCols([\"tokens\"]) \\\n",
        "    .setOutputAsArray(True)\n",
        "\n",
        "#Hashing the tokens\n",
        "hashingTF = HashingTF(inputCol=\"tokens\", outputCol=\"tf\", numFeatures=1000)\n",
        "idf = IDF(inputCol=\"tf\", outputCol=\"features\")\n",
        "\n",
        "#Classification based on the hashed tokens using the ML model Support Vector Machine\n",
        "svm = LinearSVC(featuresCol=\"features\", labelCol=\"target\")\n",
        "\n"
      ],
      "metadata": {
        "id": "8YN3lD0y38_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define the pipeline\n",
        "nlp_pipeline = Pipeline(\n",
        "    stages=[\n",
        "        document_assembler,\n",
        "        tokenizer, \n",
        "        normalizer, \n",
        "        stopwords_cleaner, \n",
        "        finisher, \n",
        "        hashingTF, \n",
        "        idf, \n",
        "        svm\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# nlp_pipeline.setStages([\n",
        "#     document_assembler,\n",
        "#     tokenizer,\n",
        "#     normalizer,\n",
        "#     stopwords_cleaner,\n",
        "#     finisher,\n",
        "#     hashingTF,\n",
        "#     idf,\n",
        "#     svm\n",
        "# ])\n",
        "\n",
        "#get the model\n",
        "p=nlp_pipeline.fit(train_set)"
      ],
      "metadata": {
        "id": "5mIMDnpp5jp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation"
      ],
      "metadata": {
        "id": "hyo7eC12EHYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(input_set):\n",
        "    results=p.transform(input_set)\n",
        "    evaluator = MulticlassClassificationEvaluator(labelCol=\"target\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "    accuracy = evaluator.evaluate(results)\n",
        "    print(\"Accuracy = %g\" % (accuracy))\n",
        "    print(\"Error = %g \" % (1.0 - accuracy))\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "-uS7iuNd4NLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(test_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VM6n46afJ92D",
        "outputId": "385a3a81-e0af-43b1-cdad-0ffabb1fee2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy = 0.688256\n",
            "Error = 0.311744 \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6882559090670315"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the model"
      ],
      "metadata": {
        "id": "tnz9mVAOEMag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p.save(\"/pipeline\")"
      ],
      "metadata": {
        "id": "9TqpjlAVKBKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "define the predict function for new instances"
      ],
      "metadata": {
        "id": "hz7BBiA5ER17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_model=PipelineModel.load(\"/pipeline\")\n",
        "\n",
        "def predict(line): # function to make a predection on a tweet or line and outout happy or sad\n",
        "    sample_df = spark.createDataFrame([[str(line)]]).toDF('tweet')\n",
        "    #-- preprocessing---\n",
        "    sample_df = sample_df.withColumn('tweet', F.regexp_replace('tweet', r'http\\S+', '')) \n",
        "    sample_df = sample_df.withColumn('tweet', F.regexp_replace('tweet', '@\\w+', '')) \n",
        "    sample_df = sample_df.withColumn('tweet', F.regexp_replace('tweet', '#', ''))\n",
        "    sample_df = sample_df.withColumn('tweet', F.regexp_replace('tweet', 'RT', ''))\n",
        "\n",
        "\n",
        "    sample_df = sample_df.withColumn('tweet', F.regexp_replace('tweet', '&amp;', ''))\n",
        "    sample_df = sample_df.withColumn('tweet', F.regexp_replace('tweet', '&quot;', ''))\n",
        "    sample_df = sample_df.withColumn('tweet', F.regexp_replace('tweet', '&gt;', ''))\n",
        "    sample_df = sample_df.withColumn('tweet', F.regexp_replace('tweet', '&lt;', ''))\n",
        "\n",
        "\n",
        "    sample_df = sample_df.withColumn('tweet', F.regexp_replace('tweet', '-', ''))\n",
        "\n",
        "    sample_df = sample_df.withColumn('tweet', F.regexp_replace('tweet', '   ', ' '))\n",
        "    sample_df = sample_df.withColumn('tweet', F.regexp_replace('tweet', '  ', ' '))\n",
        "\n",
        "    \n",
        "    #---\n",
        "    \n",
        "    result = pipeline_model.transform(sample_df)\n",
        "    sentiment = result.select('prediction').first()[0]\n",
        "    if(sentiment == 1):\n",
        "        sentiment = \"Happy\"\n",
        "        print (str(line)+ \" =====> \"+\"HAPPY\")\n",
        "    else:\n",
        "        sentiment = \"Sad\"\n",
        "        print(str(line)+ \" =====> \"+\"Sad\")\n",
        "\n",
        "    return line , sentiment"
      ],
      "metadata": {
        "id": "WjdpoLa5MXk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict(\"Iam really happy right now.\") # =>1\n",
        "predict(\"Easy Task! \")# =>1\n",
        "predict(\"I will be sad if not accepted\") #=>0\n",
        "predict(\"I am alone\")# =>0\n",
        "predict(\"My day was full of good events but at the end , a car hit me and broke my leg\")# =>0\n",
        "predict(\"Death.\") #=>0\n",
        "predict(\"I failed in my last exam\") #=>0\n",
        "predict(\"my dad bought me a new car\") #=>1\n",
        "predict(\"the new car my dad bought me was crashed :(\") #=>0\n",
        "predict(\"I am nervous\") #=>0\n",
        "predict(\"I helped many people today\") #=>1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7L1qqBvdMs8j",
        "outputId": "af9248e2-47cc-4fea-a189-68df422326ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iam really happy right now. =====> HAPPY\n",
            "Easy Task!  =====> HAPPY\n",
            "I will be sad if not accepted =====> Sad\n",
            "I am alone =====> Sad\n",
            "My day was full of good events but at the end , a car hit me and broke my leg =====> Sad\n",
            "Death. =====> HAPPY\n",
            "I failed in my last exam =====> Sad\n",
            "my dad bought me a new car =====> HAPPY\n",
            "the new car my dad bought me was crashed :( =====> HAPPY\n",
            "I am nervous =====> HAPPY\n",
            "I helped many people today =====> HAPPY\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('I helped many people today', 'Happy')"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CT1mOPPeMxkS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}